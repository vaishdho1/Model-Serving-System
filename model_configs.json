{
  "/v1/chat/gpt2": {
    "model_id": "gpt2",
    "deployment_name": "gpt2-small",
    "endpoint": "/v1/chat/gpt2",
    "max_length": 256,
    "temperature": 0.7,
    "top_p": 0.9,
    "device": "auto"
  },
  "/v1/chat/llama2": {
    "model_id": "meta-llama/Llama-2-7b-chat-hf",
    "deployment_name": "llama2-7b-chat",
    "endpoint": "/v1/chat/llama2",
    "max_length": 512,
    "temperature": 0.8,
    "top_p": 0.9,
    "device": "auto"
  },
  "/v1/chat/mistral": {
    "model_id": "mistralai/Mistral-7B-Instruct-v0.1",
    "deployment_name": "mistral-7b-instruct",
    "endpoint": "/v1/chat/mistral",
    "max_length": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "device": "auto"
  },
  "/v1/code/codellama": {
    "model_id": "codellama/CodeLlama-7b-Python-hf",
    "deployment_name": "codellama-7b-python",
    "endpoint": "/v1/code/codellama",
    "max_length": 1024,
    "temperature": 0.2,
    "top_p": 0.9,
    "device": "auto"
  },
  "/v1/chat/phi2": {
    "model_id": "microsoft/phi-2",
    "deployment_name": "phi2",
    "endpoint": "/v1/chat/phi2",
    "max_length": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "device": "auto"
  },
  "/v1/chat/tinyllama": {
    "model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "deployment_name": "tinyllama-1.1b-instruct",
    "endpoint": "/v1/chat/tinyllama",
    "max_length": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "device": "auto"
  }
}